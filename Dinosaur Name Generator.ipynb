{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Forward Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Backward Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    \n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    gradients = {}\n",
    "    \n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping Gradients to reduce exploding gradients problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipGrad(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # clip to reduce exploding gradients\n",
    "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Dinosaur Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    \"\"\"\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # representing the first character (initializing the sequence generation)\n",
    "    x = np.zeros((27, 1))\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    # idx is the index of the one-hot vector x that is set to 1, all other positions in x are zero.\n",
    "    # We will initialize idx to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step:\n",
    "    # sample a character from a probability distribution \n",
    "    # We'll stop if we reach 50 characters \n",
    "\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        idx = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "\n",
    "        indices.append(idx)\n",
    "\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Forward propagate through time \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Cliping gradients between -5 (min) and 5 (max)\n",
    "    gradients = clipGrad(gradients, maxValue = 5)\n",
    "    \n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model for name generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations = 50000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [c for c in single_example]\n",
    "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:]\n",
    "        Y.append(ix_newline)\n",
    "\n",
    "        # Performing one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        \n",
    "        \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        # Smoothing loss to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = ornithomerus\n",
      "single_example_chars ['o', 'r', 'n', 'i', 't', 'h', 'o', 'm', 'e', 'r', 'u', 's']\n",
      "single_example_ix [15, 18, 14, 9, 20, 8, 15, 13, 5, 18, 21, 19]\n",
      " X =  [None, 15, 18, 14, 9, 20, 8, 15, 13, 5, 18, 21, 19] \n",
      " Y =        [15, 18, 14, 9, 20, 8, 15, 13, 5, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.090631\n",
      "\n",
      "Jozfzlemworxgseyk\n",
      "Kizdorwukgsmwzogqxiipvfzcvaxnqrmweheuftyqphfqcjynxg\n",
      "Djvfdvcvlyebincvbiniyseugxn\n",
      "Ijaxbzlyvgipqenpdvefkuagj\n",
      "Icfpkg\n",
      "Ui\n",
      "Ozqnjrhgslrvzserpbchc\n",
      "\n",
      "\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 28.158658\n",
      "\n",
      "Us\n",
      "Mton\n",
      "Lpolntanonosaurus\n",
      "Kaenus\n",
      "Numtiatopasstelciesutius\n",
      "Ytomotudlopkikechuratotekn\n",
      "Saurus\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.815536\n",
      "\n",
      "Tishetcethenhus\n",
      "Asmusaurus\n",
      "Titan\n",
      "Pkraphiakus\n",
      "Pdosagrar\n",
      "Us\n",
      "Zkresias\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.568153\n",
      "\n",
      "Anusauaus\n",
      "A\n",
      "Ojngtos\n",
      "Inomsaurus\n",
      "Asrzosaurus\n",
      "S\n",
      "Aenosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.192668\n",
      "\n",
      "Sitafanllosaurus\n",
      "Lu\n",
      "Zhoghansaurus\n",
      "Pusisencospelumkinophibochynus\n",
      "Sicheravonia\n",
      "Monganis\n",
      "Acangsaurus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.803824\n",
      "\n",
      "Porustar\n",
      "Yanstasaurus\n",
      "Phanthibantingamarikatuloanbalus\n",
      "Uriandrodapsalaspanodopanlianmitatosaurus\n",
      "Phenustosaurus\n",
      "Kuinesaurus\n",
      "Amarosa\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.360731\n",
      "\n",
      "Vunganks\n",
      "Rtanmiratosiuropeolenis\n",
      "Kangybemalong\n",
      "Amukong\n",
      "Ynjangshodong\n",
      "Akunosaurus\n",
      "Longmisnettondypane\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.359204\n",
      "\n",
      "Ecotonatods\n",
      "Moluresiuauk\n",
      "Aokorniospacaihtis\n",
      "Dianostdeplelemasaurus\n",
      "Domonilosaurus\n",
      "Cerenong\n",
      "Akeceryt\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.158157\n",
      "\n",
      "Yiaapter\n",
      "Etiocerator\n",
      "Peoraurus\n",
      "Itosaurus\n",
      "Anclasaurus\n",
      "Serataus\n",
      "Atorosnurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.821946\n",
      "\n",
      "Ilanplophalosaurus\n",
      "Bichantosaurus\n",
      "Anzoton\n",
      "Maingosuurus\n",
      "Luialosuzhus\n",
      "Racenitophatrosaurus\n",
      "Ungondas\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.892201\n",
      "\n",
      "Cricalosaurus\n",
      "Ecgirpongiligliisaurus\n",
      "Iiicoodiosaatholuraleistagu\n",
      "Aginodonuhophylimus\n",
      "Aiptomelus\n",
      "Osaurolasaurus\n",
      "Ractosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 23.050565\n",
      "\n",
      "Arteosaurus\n",
      "Aukimamorosaurus\n",
      "Sandoseurus\n",
      "Jexyosaurus\n",
      "Fiosaurus\n",
      "Ondasodrics\n",
      "Ieinosaurus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.643479\n",
      "\n",
      "Alchansaurus\n",
      "Osazatalus\n",
      "Cronsaurus\n",
      "Aacenytor\n",
      "Urimdansaurus\n",
      "Akindanimatasaurus\n",
      "Yrssaua\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.542408\n",
      "\n",
      "Ropochkororosaurus\n",
      "Epidypanosaurus\n",
      "Boeryinsachus\n",
      "Anicanchua\n",
      "Adorxaasaurus\n",
      "Inasaururosaurus\n",
      "Apsoramcerievhageosaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.701086\n",
      "\n",
      "Onlapisaurus\n",
      "Polaramongofus\n",
      "Crimpsosaurus\n",
      "Unchinsaurus\n",
      "Olochus\n",
      "Crarirebsaurus\n",
      "Hegimisaurus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.675857\n",
      "\n",
      "Eotasaurus\n",
      "Pentimasaurus\n",
      "Envbachidosaurus\n",
      "Idylecon\n",
      "Cerosaurus\n",
      "Ngukus\n",
      "Thiumus\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.582250\n",
      "\n",
      "Dulus\n",
      "Komilmalur\n",
      "Limavosaurus\n",
      "Alesaurus\n",
      "Rosskis\n",
      "Eahiandiepis\n",
      "Loslusaurus\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.571166\n",
      "\n",
      "Uacanathosaurus\n",
      "Eyisaurus\n",
      "Ebinosaurus\n",
      "Towropaptogkinabiulosaurus\n",
      "Coeoperax\n",
      "Auanocepsaurole\n",
      "Anasconocaestomucapropenteraptus\n",
      "\n",
      "\n",
      "Iteration: 36000, Loss: 22.422967\n",
      "\n",
      "Camaeosaurus\n",
      "Jamarinosponosaurus\n",
      "Anosaurus\n",
      "Antasaurus\n",
      "Pieurosaurus\n",
      "Hersrogiptosaurus\n",
      "Golcaraptosaurus\n",
      "\n",
      "\n",
      "Iteration: 38000, Loss: 22.098128\n",
      "\n",
      "Irdondostroceras\n",
      "Rotygichurus\n",
      "Erobonycnyrosahistosaurus\n",
      "Irevrscitaton\n",
      "Kishia\n",
      "Ayakavus\n",
      "Arcyniatrieles\n",
      "\n",
      "\n",
      "Iteration: 40000, Loss: 22.288412\n",
      "\n",
      "Lendosaurus\n",
      "Rasharlilriator\n",
      "Ryosaurus\n",
      "Ispontorus\n",
      "Iscrosaurus\n",
      "Hes\n",
      "A\n",
      "\n",
      "\n",
      "Iteration: 42000, Loss: 22.381566\n",
      "\n",
      "Iaxanisithorsosrperushanoeosaurus\n",
      "Sichegia\n",
      "Amlihhaicemornathornathomersthopemmahasaurus\n",
      "Lyrapaolels\n",
      "Gychanosaurus\n",
      "Aenaraptosaurus\n",
      "Amits\n",
      "\n",
      "\n",
      "Iteration: 44000, Loss: 22.069088\n",
      "\n",
      "Asamanosaurus\n",
      "Crodoudoogrosamosaurus\n",
      "Hoseitasaurus\n",
      "Usnalgosaurus\n",
      "Bagrasaurus\n",
      "Euminzhosaurus\n",
      "Adiaeus\n",
      "\n",
      "\n",
      "Iteration: 46000, Loss: 22.071465\n",
      "\n",
      "Ianrua\n",
      "Folosaurovustaplonashaopulur\n",
      "Sirtelovus\n",
      "Aechucusphelus\n",
      "Kisidstsosaurus\n",
      "Pinopususaurus\n",
      "Locoivenator\n",
      "\n",
      "\n",
      "Iteration: 48000, Loss: 22.289064\n",
      "\n",
      "Rualasaurus\n",
      "Titoryosaurus\n",
      "Ikovelopeltosaurus\n",
      "Iatononosaurus\n",
      "Ylidioodosterua\n",
      "Isaurianinokosaurus\n",
      "Iownuuauritoonon\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
